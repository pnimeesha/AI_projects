{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32448200-82ae-41e2-b353-a5bb96a3e9fa",
   "metadata": {},
   "source": [
    "# Create Training Dataset for Embedding Fine-tuning\n",
    "\n",
    "Based on example from [here](https://sbert.net/docs/sentence_transformer/training_overview.html#trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c387ad9-14db-46eb-8671-94d2e3f2eb88",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "193d45ec-ce81-4828-95d4-28fa6fba139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datasets import DatasetDict, Dataset\n",
    "import re\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from functions import remove_irrelevant_sections, extract_qualifications_from_html, remove_eoe_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b80c77-27ca-4950-8317-c0142f2e7706",
   "metadata": {},
   "source": [
    "### load data (batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e12730e-e2a2-4a88-a72e-981a2dfb2b48",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # extract JDs\n",
    "# df_jobs = pd.read_csv(\"data/job_data.csv\")\n",
    "# # df_jobs = df_jobs.drop_duplicates()\n",
    "\n",
    "# # only keep text relevant to job qualifications\n",
    "# df_jobs['description_cleaned'] = df_jobs['description'].apply(remove_irrelevant_sections)\n",
    "# df_jobs['description_cleaned'] = df_jobs['description_cleaned'].apply(extract_qualifications_from_html)\n",
    "# df_jobs['description_cleaned'] = df_jobs['description_cleaned'].apply(remove_eoe_notes)\n",
    "\n",
    "# # store job descriptions in a list\n",
    "# job_description_list = df_jobs['description_cleaned'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b99799d-db3c-4007-9878-8ae116186430",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # extract synthetic queries and store in list (from batch request_\n",
    "# file_path = 'data/output.jsonl'\n",
    "# query_list = []\n",
    "\n",
    "# with open(file_path, 'r') as file:\n",
    "#     for line in file:\n",
    "#         query = json.loads(line)['response']['body']['choices'][0]['message']['content'].replace('\"', '')\n",
    "#         query_list.append(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8faa3f5-acda-4685-bb4f-1d47c96281bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # create dict with queries and JDs\n",
    "# df = pd.DataFrame({\"query\" : query_list, \"job_description_pos\" : job_description_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe9291e-6089-42e5-9d0a-65b314677f9e",
   "metadata": {},
   "source": [
    "### load data (from csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e074543-057d-4f98-a273-9f624a8438f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract JDs\n",
    "df_jobs = pd.read_csv(\"data/job_data_w_query.csv\")\n",
    "\n",
    "# only keep text relevant to job qualifications\n",
    "df_jobs['job_description_pos'] = df_jobs['description'].apply(remove_irrelevant_sections)\n",
    "df_jobs['job_description_pos'] = df_jobs['job_description_pos'].apply(extract_qualifications_from_html)\n",
    "df_jobs['job_description_pos'] = df_jobs['job_description_pos'].apply(remove_eoe_notes)\n",
    "\n",
    "# store job descriptions in a list\n",
    "df = df_jobs[['query', 'job_description_pos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf7b999e-67f5-41c4-89f3-b97cc5f1835a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (1179, 2)\n",
      "Unique JDs: (1020, 2)\n",
      "Unique queries: (1012, 2)\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates\n",
    "print(\"Original shape:\", df.shape)\n",
    "df = df.drop_duplicates(subset=['job_description_pos'])\n",
    "print(\"Unique JDs:\", df.shape)\n",
    "df = df.drop_duplicates(subset=['query'])\n",
    "print(\"Unique queries:\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f045de11-6ea2-46c1-8c30-de1f3db4a3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>job_description_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kafka data injection, Snowflake SQL optimizati...</td>\n",
       "      <td>experience neededVery strong experience in Kaf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>automotive engineering, cloud infrastructure m...</td>\n",
       "      <td>requirements to determine feasibility of desig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>React development, API authentication, AWS Lambda</td>\n",
       "      <td>experienceAccountable for code quality, includ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst contract Queens NY, data modeling...</td>\n",
       "      <td>QualificationsAnalytical skills, including the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>job search query: mortgage banking systems opt...</td>\n",
       "      <td>requirements and industry practices for mortga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  Kafka data injection, Snowflake SQL optimizati...   \n",
       "1  automotive engineering, cloud infrastructure m...   \n",
       "2  React development, API authentication, AWS Lambda   \n",
       "3  Data Analyst contract Queens NY, data modeling...   \n",
       "4  job search query: mortgage banking systems opt...   \n",
       "\n",
       "                                 job_description_pos  \n",
       "0  experience neededVery strong experience in Kaf...  \n",
       "1  requirements to determine feasibility of desig...  \n",
       "2  experienceAccountable for code quality, includ...  \n",
       "3  QualificationsAnalytical skills, including the...  \n",
       "4  requirements and industry practices for mortga...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d654eb-6972-410c-994f-7c9f9d6f5cce",
   "metadata": {},
   "source": [
    "### create negative pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bedfc469-fb7e-47f5-8ec2-c87a7896daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec5cf0b-bc5d-4ca8-be62-a58b0bbaa6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1012, 768)\n",
      "CPU times: user 18.5 s, sys: 13.4 s, total: 32 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Encode all job descriptions\n",
    "job_embeddings = model.encode(df['job_description_pos'].to_list())\n",
    "print(job_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e161aed6-0aa5-4c52-8c52-c4956fe10f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1012, 1012])\n"
     ]
    }
   ],
   "source": [
    "# compute similarities\n",
    "similarities = model.similarity(job_embeddings, job_embeddings)\n",
    "print(similarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed79d19a-d75b-45d6-bb48-ad00cbd69c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match least JDs least similar to positive match as the negative match\n",
    "similarities_argsorted = np.argsort(similarities.numpy(), axis=1)\n",
    "negative_pair_index_list = []\n",
    "\n",
    "for i in range(len(similarities)):\n",
    "\n",
    "    # Start with the smallest similarity index for the current row\n",
    "    j = 0\n",
    "    index = int(similarities_argsorted[i][j])\n",
    "\n",
    "    # Ensure the index is unique\n",
    "    while index in negative_pair_index_list:\n",
    "        j += 1  # Move to the next smallest index\n",
    "        index = int(similarities_argsorted[i][j])  # Fetch next smallest index\n",
    "\n",
    "    negative_pair_index_list.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1828098c-a0b5-4fef-a2d5-50f284d866f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add negative pairs to df\n",
    "df['job_description_neg'] = df['job_description_pos'].iloc[negative_pair_index_list].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22900620-acd2-4dde-b7d6-cdcf89c80643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>job_description_pos</th>\n",
       "      <th>job_description_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kafka data injection, Snowflake SQL optimizati...</td>\n",
       "      <td>experience neededVery strong experience in Kaf...</td>\n",
       "      <td>qualifications, skills, competencies, competen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>automotive engineering, cloud infrastructure m...</td>\n",
       "      <td>requirements to determine feasibility of desig...</td>\n",
       "      <td>SQL (expert)Snowflake - not a roadblock (added...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>React development, API authentication, AWS Lambda</td>\n",
       "      <td>experienceAccountable for code quality, includ...</td>\n",
       "      <td>Resource should be able to visualize and expla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst contract Queens NY, data modeling...</td>\n",
       "      <td>QualificationsAnalytical skills, including the...</td>\n",
       "      <td>experiences. We own and operate leading entert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>job search query: mortgage banking systems opt...</td>\n",
       "      <td>requirements and industry practices for mortga...</td>\n",
       "      <td>Qualifications:\\nFluency in English (native or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  Kafka data injection, Snowflake SQL optimizati...   \n",
       "1  automotive engineering, cloud infrastructure m...   \n",
       "2  React development, API authentication, AWS Lambda   \n",
       "3  Data Analyst contract Queens NY, data modeling...   \n",
       "4  job search query: mortgage banking systems opt...   \n",
       "\n",
       "                                 job_description_pos  \\\n",
       "0  experience neededVery strong experience in Kaf...   \n",
       "1  requirements to determine feasibility of desig...   \n",
       "2  experienceAccountable for code quality, includ...   \n",
       "3  QualificationsAnalytical skills, including the...   \n",
       "4  requirements and industry practices for mortga...   \n",
       "\n",
       "                                 job_description_neg  \n",
       "0  qualifications, skills, competencies, competen...  \n",
       "1  SQL (expert)Snowflake - not a roadblock (added...  \n",
       "2  Resource should be able to visualize and expla...  \n",
       "3  experiences. We own and operate leading entert...  \n",
       "4  Qualifications:\\nFluency in English (native or...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857eae3-19e1-4e75-af14-62cbd9fb0dd0",
   "metadata": {},
   "source": [
    "### train-eval-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "992598e2-fd59-4fb7-b548-86aa6bd3a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into train, validation, and test sets (e.g., 80% train, 10% validation, 10% test)\n",
    "train_frac = 0.8\n",
    "valid_frac = 0.1\n",
    "test_frac = 0.1\n",
    "\n",
    "# define train and validation size\n",
    "train_size = int(train_frac * len(df))\n",
    "valid_size = int(valid_frac * len(df))\n",
    "\n",
    "# create train, validation, and test datasets\n",
    "df_train = df[:train_size]\n",
    "df_valid = df[train_size:train_size + valid_size]\n",
    "df_test = df[train_size + valid_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8afa6e-cf80-42a7-9a5c-9d611f057211",
   "metadata": {},
   "source": [
    "### upload to hugging face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f342bad7-6221-4955-acb5-189efc3b18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas DataFrames back to Hugging Face Datasets\n",
    "train_ds = Dataset.from_pandas(df_train)\n",
    "valid_ds = Dataset.from_pandas(df_valid)\n",
    "test_ds = Dataset.from_pandas(df_test)\n",
    "\n",
    "# Combine into a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_ds,\n",
    "    'validation': valid_ds,\n",
    "    'test': test_ds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4be4fbc-105f-41ab-b10d-6dfa88f9c519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query', 'job_description_pos', 'job_description_neg'],\n",
       "        num_rows: 809\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['query', 'job_description_pos', 'job_description_neg'],\n",
       "        num_rows: 101\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query', 'job_description_pos', 'job_description_neg'],\n",
       "        num_rows: 102\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e5a6e0-ec9f-4659-979a-d9f5c01475e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26de8f55857e4a2eadfec8b6041283d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55b0bc3888a4a8da3c425f33ec2ddca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c898f15d4448a298a720f06ded24c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cda7ef3fc94cb39164375116584cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f581c003870423fa49ea7daa4d75428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e16a3ee1d6148e6803aee727bda557e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/shawhin/ai-job-embedding-finetuning/commit/c86ac36bb69d9cfa0f85968382b58e1be707f85b', commit_message='Upload dataset', commit_description='', oid='c86ac36bb69d9cfa0f85968382b58e1be707f85b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/shawhin/ai-job-embedding-finetuning', endpoint='https://huggingface.co', repo_type='dataset', repo_id='shawhin/ai-job-embedding-finetuning'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push data to hub\n",
    "dataset_dict.push_to_hub(\"pnimeesha/ai-job-embedding-finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49521b7-fa5e-426d-9056-9ed8206abe28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
